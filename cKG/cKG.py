# -*- coding: utf-8 -*-
"""
Created on Thu Dec  6 14:11:36 2018
This script is developed base on the learn_coregion.py.
We use the cmisoKG to solve the bayesian optimization problem.
Here we deal with the rosenbrock funciton with constraint.
The detail refers to the note miso_with_constraints.

@author: 44266
"""
import logging
import pickle
import os
from copy import deepcopy
import GPy
from GPy.util.linalg import dpotrs, dpotri, symmetrify, \
jitchol, dtrtrs
from GPy.util import diag
import numpy as np
from pyDOE import lhs
from scipy.stats import norm
from paramz import ObsAr

def main(num=1000, num_train=10, num_h=2, tau=3000, total=300, spl_num=10, \
         num_k=5, fname=os.getcwd(), status='start', func="rosen"):
    np.set_printoptions(linewidth=150)
    logger = logging.getLogger('main.cKG')
    logger.info('cKG begins at num = %s, num_train = %s, num_h = %s, '
                'tau = %s, total = %s, spl_num = %s, num_k = %s func = %s\n'
                , num, num_train, num_h, tau, total, spl_num, num_k, func)
    if status == 'start':
        myPara, fD = init_Para_fD(num, tau, num_h, spl_num, num_train, func, fname)
    else:
        with open(fname + '/data.pkl', 'rb') as f:
            myPara, fD = pickle.load(f)
    for _ in range(total):
        m, fD = setup_model(fD, func)
        myPara.update(fD)
        logger.info('Number of sampled points: %s', myPara.pred_var.shape[0])
        un_star = get_un_star(fD, myPara, m, spl_num)
        En_un_1_set = get_En_un_1_star_fast(fD, myPara, m, num_k)
        logger.debug('sampled f point and value is\n %s \n', \
                     np.hstack([fD['f'].X, fD['f'].obs_val]))
        fD = Get_min_cKG(fD, myPara, En_un_1_set, un_star, func)
        with open(fname + '/data.pkl', 'wb') as f:
            pickle.dump([myPara, fD], f)
def init_Para_fD(num, tau, num_h, spl_num, num_train, func, fname):
    '''
    Initialize the variable myPara and fD for storing all varaibles
    used in this files.
    '''
    myPara = SampleParams(num, tau, num_h, spl_num, num_train, func, fname)
    fD = {'f': None, 'c1': None}
    for k in fD.keys():
        fD[k] = Func_Dict(myPara.X_prd, num_train, k)
    return myPara, fD

def update_fD(m, fD1, myPara1, task):
    '''
    Make prediction for each point, Update fD.
    '''
    for ea1 in fD1.keys():
        if task == 'var' and ea1 != 'f':
            fD1[ea1].Kx_set_n1 = m.kern.K(myPara1.pred_var, fD1[ea1].X_prd)
            Kx = m.kern.K(myPara1.pred_var, fD1[ea1].X_prd)
            Kxx = m.kern.Kdiag(fD1[ea1].X_prd)
            Lw = myPara1.Lw_n1
            var = predict_chol_var(Kx, Lw, Kxx, noise=1e-6)
            var = (var - fD1[ea1].noise)* fD1[ea1].nmlz ** 2
            fD1[ea1].var_prd = var.clip(min=0) + fD1[ea1].noise
        elif task == 'mean' and ea1 == 'f':
            Lw = myPara1.Lw_n1
            Kx = m.kern.K(myPara1.pred_var, fD1[ea1].X_prd)
            Y = myPara1.Y
            mean = predict_chol_mean(Kx, Lw, Y)
            fD1[ea1].mean_prd = mean * fD1[ea1].nmlz + fD1[ea1].obs_mean
        else:
            pass
    return fD1
def get_En_un_1_star_fast(fD, myPara, m, num_k=5):
    '''
    Get the Expected utility after making a imaginary sample
    '''
    En_un_1_set = {'f':None, 'c1':None}
    for ea in fD.keys():
        En_un_1_set[ea] = Eval_f(myPara.num)
        spl_set_En = CRN_gen(fD, ea, num_k)
        myPara1, fD1 = deepcopy(myPara), deepcopy(fD)
        for ind, spl_ind in enumerate(spl_set_En):
            #print(str(ind)+'/'+str(myPara1.num)+' in task '+ea)
            spl_x1 = np.array([fD[ea].X_prd[ind]])
            myPara1.get_Lw_n1(m, myPara, spl_x1)
            fD1 = update_fD(m, fD1, myPara1, 'var')
            myPara1.get_Lw_set(m, myPara, fD1)
            En_un_1_set[ea].val[ind] = Aver_En_un_1_samples(m, fD1, myPara, myPara1, spl_ind[0])
    logger = logging.getLogger('main.cKG')
    logger.debug("spl_set_En dimension is %s", spl_set_En.shape)
    return En_un_1_set
def Aver_En_un_1_samples(m, fD1, myPara, myPara1, samples):
    '''
    Outer loop, Averaging Expected utility generated by samples.
    '''
    un_1_star_sum, count = 0, 0
    for Y_ind in samples:
        myPara1.Y = np.vstack([myPara.Y, Y_ind])
        fD1 = update_fD(m, fD1, myPara1, 'mean')
        # compute the un+1*
        un_1_star = get_un_star_fast(fD1, myPara1)
        un_1_star_sum += un_1_star
        count += 1
    # average the un+1* in spl_set
    En_un_1_star = un_1_star_sum/count
    return En_un_1_star

def get_un_star_fast(fD, myPara):
    '''
    Get the utility for every point. It is a fast version.
    '''
    spl_set = CRN_gen(fD, 'c1', myPara.spl_num)
    Pr_feasible = -norm.cdf(0, loc=fD['c1'].mean_prd, scale=np.sqrt(fD['c1'].var_prd))+1
    # get E_n{g(x)|x is feasible}
    un_set = np.zeros((myPara.num, 1))
    E_n_condi = np.zeros((myPara.num, 1))
    for j in range(myPara.num):
        Lw = myPara.Lw_set[j]
        Kx_plus = myPara.Kx_set[j]
        un_set, _ = Aver_Un_star_samples(un_set, E_n_condi, j, myPara, \
                                        spl_set, Lw, Kx_plus, Pr_feasible, fD)
    un_star = np.min(un_set)
    return un_star
def Aver_Un_star_samples(un_set, E_n_condi, j, myPara, spl_set, Lw, Kx, Pr_feasible, fD):
    '''
    Inner loop, averaging Expected utility generated by samples.
    '''
    obj_pos, count = 0, 0
    for k in range(myPara.spl_num):
        spl_c = spl_set[j][0][k]
        Y = np.vstack([myPara.Y, spl_c])
        if spl_c * fD['c1'].nmlz >= 0:# if c>0, update the model with it
            mean = predict_chol_mean(Kx, Lw, Y)
            mean = mean * fD['f'].nmlz + fD['f'].obs_mean
            obj_pos += mean
            count += 1
    un_set[j] = myPara.tau
    if count > 0:
        E_n_condi[j] = obj_pos/count
        un_set[j] = Pr_feasible[j][0] * E_n_condi[j]+myPara.tau * (1-Pr_feasible[j][0])
    return un_set, E_n_condi
def predict_chol_mean(Kx, Lw, Y):
    '''
    Get the mean with cholesky decomposition Lw
    '''
    alpha, _ = dpotrs(Lw, Y, lower=1)
    mean = np.matmul(Kx.T, alpha)
    return mean
def predict_chol_var(Kx, Lw, Kxx, noise=1e-6):
    '''
    Get the variance with cholesky decomposition Lw
    '''
    Mywoodbury_inv, _ = dpotri(Lw, lower=1)
    symmetrify(Mywoodbury_inv)
    tmp = dtrtrs(Lw, Kx)[0]
    Myvar = (Kxx - np.square(tmp).sum(0))[:, None] + noise
    return Myvar
def CRN_gen(fD, task, spl_num):
    '''
    it use common random number replace posterior_samples:
    '''
    mean = fD[task].mean_prd
    std = np.sqrt(fD[task].var_prd)
    rand_norm = np.array([np.random.normal(size=spl_num)])
    spl_crn = np.expand_dims(np.matmul(std, rand_norm)+mean, axis=1)
    return (spl_crn - fD[task].obs_mean)/fD[task].nmlz
def setup_model(fD, func, noise=1e-6):
    for ea in fD.keys():
        fD[ea].noise = noise
        fD[ea].obs_val = obj_func(fD[ea].X, func)[ea][:, None]
        fD[ea].obs_mean = np.mean(fD[ea].obs_val)
        fD[ea].nmlz = fD[ea].obs_val.std(0)
        fD[ea].Ny = (fD[ea].obs_val - fD[ea].obs_mean)/fD[ea].nmlz
    Ny = [fD['f'].Ny, fD['c1'].Ny]
    K = GPy.kern.Matern52(input_dim=2, ARD=True)
    icm = GPy.util.multioutput.ICM(input_dim=2, num_outputs=2, kernel=K)
    m = GPy.models.GPCoregionalizedRegression([fD['f'].X, fD['c1'].X], Ny, kernel=icm)
    m['.*Mat52.var'].constrain_fixed(1.)
    m['.*Gaussian_noise'].constrain_fixed(noise)
    try:
        m.optimize(optimizer='lbfgsb')
    except Exception:
        m.optimize(optimizer='scg')
    m = m.copy()
    for k in fD.keys():
        mean, var = m.predict(fD[k].X_prd, Y_metadata=fD[k].noise_dict)
        fD[k].mean_prd = mean * fD[k].nmlz + fD[k].obs_mean
        fD[k].var_prd = var - 2 * fD[k].noise
        fD[k].var_prd = fD[k].var_prd.clip(min=0)
        fD[k].var_prd = fD[k].var_prd * fD[k].nmlz**2 + 2 * fD[k].noise
    return m, fD
def Get_min_cKG(fD, myPara, En_un_1_set, un_star, func):
    '''
    compute the minimal cKG and update fD.
    '''
    logger = logging.getLogger('main.cKG')
    min_cKG = np.inf
    for ea in fD.keys():
        En_set = En_un_1_set[ea].val - un_star
        En_set[fD[ea].X_ind[:, 0].tolist()] = np.inf
        min_val = min(En_set)
        ind = np.unravel_index(np.argmin(En_set, axis=None), En_set.shape)[0]
        if min_val <= min_cKG:
            min_cKG = min_val
            min_ind = ind
            min_task = ea
    fD[min_task].X_ind = np.vstack([fD[min_task].X_ind, min_ind])
    if min_task == 'f':
        myPara.X_ind = np.vstack([myPara.X_ind, np.array([[min_ind, 0]])])
    else:
        myPara.X_ind = np.vstack([myPara.X_ind, np.array([[min_ind, 1]])])
    try:
        spl_pt = np.array([fD[min_task].X_prd[min_ind]])
    except:
        raise UnboundLocalError('the cKG computation fails')
    fD[min_task].X = np.vstack([fD[min_task].X, spl_pt[:, 0:-1]])
    logger.info("The Next Sample task is {} at point {} cKG is {}"\
                .format(min_task, str(spl_pt[0, 0:-1]), min_cKG[0]))
    logger.info("The obj value is %s, feasiblility is %s", \
                obj_func(spl_pt, func)['f'][0], (obj_func(spl_pt, func)['c1'][0] > 0))
    logger.info("The feasible probability is %s", \
                -norm.cdf(0, loc=fD['c1'].mean_prd[min_ind], \
                          scale=np.sqrt(fD['c1'].var_prd[min_ind]))+1)
    logger.info("Evaluate f {} times, c1 {} times\n"\
                .format(fD['f'].X.shape[0], fD['c1'].X.shape[0]))
    return fD
def get_un_star(fD, myPara, m, spl_num):
    '''
    Get the current best point without imaginary sample.
    '''
    spl_set = CRN_gen(fD, 'c1', spl_num)
    Pr_feasible = -norm.cdf(0, loc=fD['c1'].mean_prd, scale=np.sqrt(fD['c1'].var_prd))+1
    # get E_n{g(x)|x is feasible}
    un_set = np.zeros((myPara.num, 1))
    E_n_condi = np.zeros((myPara.num, 1))
    logger = logging.getLogger('main.cKG')
    logger.debug('Pr_feasible dimmension is %s, max %s min %s', \
                 Pr_feasible.shape, np.max(Pr_feasible), np.min(Pr_feasible))
    logger.debug('spl_set dimmension is %s', spl_set.shape)
    for j in range(myPara.num):
        spl_x1 = np.array([fD['c1'].X_prd[j]])
        spl_x2 = np.array([fD['f'].X_prd[j]])
        Lw = jitchol_plus(m, myPara.pred_var, m.posterior._K, spl_x1, fD['f'].noise)
        pred_var = np.vstack([myPara.pred_var, spl_x1])
        Kx_plus = m.kern.K(pred_var, spl_x2)
        un_set, E_n_condi = \
        Aver_Un_star_samples(un_set, E_n_condi, j, myPara, spl_set, Lw, Kx_plus, Pr_feasible, fD)
    un_star = np.min(un_set)
    ind = np.unravel_index(np.argmin(un_set, axis=None), un_set.shape)[0]
    logger.info("un_star is %s at point %s, condition expected mean %s, posterior mean %s,variance %s, feasible probability %s"
                , un_star, myPara.X_prd[ind, :], E_n_condi[ind, 0], fD['f'].mean_prd[ind, 0], fD['f'].var_prd[ind, 0], Pr_feasible[ind, 0])
    logger.debug("Coordinate (2D), f predict mean, var, un_set, E_n_condi, feasiblity, f predict mean use K_inv is \n%s\n"
                 , np.hstack([myPara.X_prd, fD['f'].mean_prd, fD['f'].var_prd, un_set, E_n_condi, Pr_feasible])[0:20, :])
    logger.debug("K max and min is %s", [np.max(m.posterior._K), np.min(m.posterior._K)])
    return un_star
def jitchol_plus(m, pred_var, cov, X_new, noise=1e-6):
    '''
    Add new points to form new matrix: add columns and rows to covariance Matrix,
    Return the cholesky decomposition lower triangular
    '''
    Kx = m.kern.K(pred_var, X_new)
    kxx = m.kern.K(X_new)
    K_plus = (np.hstack([np.vstack([cov, Kx.T]), \
                       np.vstack([Kx, kxx])]))
    diag.add(K_plus, noise*np.ones(K_plus.shape[0])+1e-8)
    Lw = jitchol(K_plus)
    return Lw
class Func_Dict(object):
    '''
    The class of fD, store the function sample value and position.
    '''
    def __init__(self, X_prd, num_train, task):
        self.X = X_prd[0:num_train, :]
        self.X_ind = np.array([range(num_train)]).T
        # prediction need to add extra colloum to X_prd to select predicted function
        if task == 'f':
            self.X_prd = np.hstack([X_prd, np.zeros_like(X_prd)[:, 0][:, None]])
        else:
            self.X_prd = np.hstack([X_prd, np.ones_like(X_prd)[:, 0][:, None]])
        self.noise_dict = {'output_index':self.X_prd[:, 2:].astype(int)}

class Eval_f(object):
    def __init__(self, num):
        self.val = np.zeros((num, 1))

class SampleParams(object):
    '''
    The class of Mypara, storing most parameters.
    '''
    def __init__(self, num, tau, num_h, spl_num, num_train, func, fname):
        self.num = num
        self.tau = tau
        self.num_h = num_h
        self.spl_num = spl_num
        self.num_train = num_train
        for _ in range(int(fname[-1])+1):
            if func == "rosen":
                self.X_prd = lhs(2, samples=num)*4-2
            else:
                lhs_s = lhs(2, samples=num)
                self.X_prd = np.hstack([np.array([15*lhs_s[:, 0]-5]).T, \
                                        np.array([15*lhs_s[:, 1]]).T])
        ind = np.array([range(num_train)]).T
        ind1 = np.hstack([ind, np.zeros_like(ind)[:, 0][:, None]])
        ind2 = np.hstack([ind, np.ones_like(ind)[:, 0][:, None]])
        self.X_ind = np.vstack([ind1,ind2])
    def update(self, fD):
        self.Y = np.vstack([fD['f'].Ny, fD['c1'].Ny])
        self.pred_var = ObsAr(np.vstack([add_col(fD['f'].X, 0), add_col(fD['c1'].X, 1)]))
    def get_Lw_n1(self, m, myPara, spl_x1):
        '''
        Get Lw_set and pred_var base on n+1 observation.
        '''
        cov = m.posterior._K
        self.Lw_n1 = jitchol_plus(m, myPara.pred_var, cov, spl_x1)
        self.pred_var = np.vstack([myPara.pred_var, spl_x1])
    def get_Lw_set(self, m, myPara, fD1):
        '''
        Get Lw_set and Kx_set base on n+2 observation.
        '''
        #dim = self.pred_var.shape[0]+1        
        self.Lw_set = np.zeros(self.num, dtype=np.ndarray)
        self.Kx_set = np.zeros(self.num, dtype=np.ndarray)
        for l in range(self.num):
            spl_x = np.array([fD1['c1'].X_prd[l]])
            spl_x1 = np.array([fD1['f'].X_prd[l]])
            Xnew = np.vstack([myPara.pred_var[-1, :][np.newaxis], spl_x])            
            self.Lw_set[l] = jitchol_plus(m, myPara.pred_var, m.posterior._K, Xnew)
            temp_pred_var = np.vstack([self.pred_var, spl_x])
            self.Kx_set[l] = m.kern.K(temp_pred_var, spl_x1)
def add_col(X, num=0):
    if num == 0:
        X = np.hstack([X, np.zeros_like(X)[:, 0][:, None]])
    else:
        X = np.hstack([X, np.ones_like(X)[:, 0][:, None]])
    return X
def predict_chol(m, X_new, noise=1e-6):
    '''
    This method extract from exact_gaussian_inference.py line 55 - 59. Imported in gp.py.
    input:
        m: GP model
        X_new: M*3 ndarray, predicted position
    variables:
        LW: N*N ndarray, cholesky decompostiion of K, LW = jitchol(K)
        Kx: N*M ndarray, covariance column between M new points and N samples,
            Kx = m.kern.K(pred_val, Xnew)
            obs_mean: int
            nmlz: int
    output:
        mean, var: M*1 ndarray, predicted mean and variance
    '''
    Kx = m.kern.K(m.X, X_new)
    Ky = m.posterior._K.copy()
    diag.add(Ky, noise*np.ones(Ky.shape[0])+1e-8)
    #Wi, LW, LWi, W_logdet = pdinv(Ky)
    LW = jitchol(Ky)    
    alpha, _ = dpotrs(LW, m.Y, lower=1)
    mean = np.matmul(Kx.T, alpha)
    Mywoodbury_inv, _ = dpotri(LW, lower=1)
    symmetrify(Mywoodbury_inv)
    Kxx = m.kern.Kdiag(X_new)
    tmp = dtrtrs(LW, Kx)[0]
    Myvar = (Kxx - np.square(tmp).sum(0))[:, None] + noise
    return mean, Myvar
def obj_func(params, func="rosen"):
    '''
    The objtive and constraint function for cKG:
    Rosenbrock and branin.
    '''
    if func == "rosen":
        x1 = params[:, 0]
        x2 = params[:, 1]
        a = 1
        b = 100
        c1 = -x1**2 - (x2-1)**2/2 + 2
        f = (a - x1)**2 + b*(x2 - x1**2)**2
    else:
        x1 = params[:, 0]
        x2 = params[:, 1]
        a = 1
        b = 5.1/(4*np.pi**2)
        c = 5/np.pi
        r = 6
        s = 10
        t = 1/(8*np.pi)
        f = a*(x2 - b*x1**2 +c*x1 - r)**2 + s*(1-t)*np.cos(x1) + s
        c1 = 2. - a*(x2 - b*x1**2 + c*x1 - r)**2
    return {'f':f, 'c1':c1}
if __name__ == '__main__':
    main()
    